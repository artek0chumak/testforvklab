# Отчет о проделанной работе.

## Контест и данные

Контест: multilabel классификация текста. Label шесть, данных, которые отмечены нулевым вектором, достаточно много: 0.9 от всего датасета. Распределения label от длины(в ед. токенов) не сильно отличается (ноутбук LookAtData). Текст на английском языке.

## Классификация текста

Для классификации текстов используются разные модели, от Tf-Idf с классификатором, RNN, CNN, трансформеры. Главный критерий для нас: вес модели и её скорость работы. В последнее время sota-моделями являются трансформеры, которые я использую в этой работе. Её скорость зависит от размера последовательности в квадрате, но её можно распараллелить, в отличии от LSTM. Также плюс трансформера в интерпретируемости модели, механизм внимания позволяет понять, на какие слова модель обращает внимание.

В качестве нижней оценки качества я использую классическую модель: Tf-Idf с логистической регрессией(ноутбук baseline). Качество такого pipeline:

| | |
|-----------------|---------|
|precision micro  | 0.17    |
|precision macro  | 0.15    |
|recall micro     | 0.81    |
|recall macro     | 0.81    |
|f1 micro         | 0.28    |
|f1 macro         | 0.23    |
|roc_auc micro    | 0.82    |
|roc_auc macro    | 0.83    |

## Как уменьшить нейронную сеть
Существуют различные способы уменьшения модели, как и количества параметров, так и общий вес:

1. Обучать с числами двойной точности, после обучения использовать числа с одинарной точностью. Такая техника применяется для загрузки моделей в мобильные телефоны.
2. После обучения определить элементы матрицы в слоях, не сильно влияющие на конечный результат. Обнулить их.(Sparse networks)
3. Использовать полноразмерную модель для обучения модели поменьше. Knowledge Distilation.

Исходя из имеющихся ресурсов и времени, а также после просмотра ноутбуков на kaggle(где большинство использовали относительно простые модели), я подумал, что можно реализовать следующее: найти минимальное число параметров, при котором модель уже достаточно хорошо(хотя бы в 2 раза выше метрики baseline) решает эту задачу. Большие модели, как BERT или GPT-2, могут решать многие задачи, из-за этого у них много параметров и дольше обучение. Минусы этого подхода: модель может не подойти под решения других проблем, недообучение, модель может труднее сходится к оптимальному.

## Модель №1

### Предобработка данных

Вместо обычно используемых word2vec как Glove и FastText, занимающие много памяти, я решил использовать свой, с меньшим словарем. Для этого я применяю BPE кодировщик. Также происходит очистка текста от знаков препинания и цифр, оставляю только слова(просто применяю `re.findall(r'[\a-zA-Z]+')`).

### Трансформер

Для трансформера я использую код из [этого ресурса](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec). Для классификации потребуется только Encoder для вытаскивания фич.

В [статье](https://arxiv.org/pdf/1508.04025.pdf) показаны различные реализации механизма внимания (3 стр.). Для уменьшения количества параметров я меняю реализацию score функции на dot.

### Обучение

Для ускорения обучения я использую последовательность длинной 200 токенов. Это 0.9 датасета, поэтому это не должно сказаться на обучении. 

Для работы с имбалансностью датасета используется подсчет весов при обучении.

В осносном обучение происходило при 30-100 эпохах, для каждой модели подбирался learning rate(в пределах $10^{-4}-10^{-6}$), который линейно уменьшался во время обучения.

## Результат

Маленьким моделям не удается быть лучше чем baseline:

|||
|-----------------|---------|
|precision micro  | 0.0914  |
|precision macro  | 0.0379  |
|recall micro     | 0.4392  |
|recall macro     | 0.1771  |

Использовались модели с небольшим размерами матриц(<20). Количество параметром было в пределе 200К. Ноутбук, используемый в эксперименте, прилагается(BaseAttention).

## Вывод
Как показывает эксперимент, для решения этой задачи не достаточно использовать маленькие модели, обученные с нуля. Возможно, методы оптимизации большим моделей лучше покажут себя при решении этой задачи.

## Модель №2

### Проверка Embeddings

После получения неудовлетворительных результатов первой модели, была предпринята попытка предварительно обучить embeddings на тренировочном датасете. Как показали эксперименты, не удалось добиться улучшения результата. Для обучения embeddings использовалась skip-gram модель для (2, 3)-граммах. Требуется, либо использование большего датасета для предобучения, либо увеличения n в n-граммах, что потребует большего времени предобучения.

### Увеличение модели

Из-за невозможности использовать BPE кодирование для уменьшения словаря, я пришел к выводу использования Glove. Используется наименьшее векторное представление слов: 50-мерное. 

Использование Glove приводит к 100 кратному увеличению количества параметров при неполном словаре(использовались только слова из датасета). Но модель лучше обучается и показывает следующие результаты:

| | |
|-----------------|---------|
|precision micro  | 0.79    |
|precision macro  | 0.48    |
|recall micro     | 0.62    |
|recall macro     | 0.38    |
|f1 micro         | 0.69    |
|f1 macro         | 0.42    |
|roc_auc micro    | 0.80    |
|roc_auc macro    | 0.68    |

В целом, модель работает лучше чем baseline. При более тонком подборе параметров можно получить результаты, более близкие к верхним строчкам в соревновании.

Эксперимент находится в ноутбуке BestResult, там же определенны параметры модели. Посмотрим на метрики модели от label объекта:

|      label   |precision |   recall | f1-score | support|
|--------------|----------|----------|----------|--------|
|        toxic |     0.81 |    0.69  |   0.75   |  2965  |
| severe_toxic |     0.48 |    0.29  |   0.36   |   300  |
|     obscene  |     0.83 |    0.69  |   0.75   |  1592  |
|       threat |     0.00 |    0.00  |   0.00   |    89  |
|       insult |     0.74 |    0.62  |   0.67   |  1515  |
|identity_hate |     0.00 |    0.00  |   0.00   |   270  |

По таблице видно, что модель плохо определяет threat и identity_hate. Для решения этой проблемы можно увеличить количество объектов этого класса.

### Вывод

Текущая модель лучше справляется с этой задачей. Изменение с предыдущей моделью в использовании предобученных word2vec, что дает возможность в использовании BPE кодировки при большем датасете и предварительном обучении embedding слоя.

