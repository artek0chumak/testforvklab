# Отчет о проделанной работе.

## Контест и данные

Контест: multilabel классификация текста. Label шесть, данных, которые отмечены нулевым вектором, достаточно много: 0.9 от всего датасета. Распределения label от длины(в ед. токенов) не сильно отличается (ноутбук LookAtData). Текст на английском языке.

## Классификация текста

Для классификации текстов используются разные модели, от Tf-Idf с классификатором, RNN, CNN, трансформеры. Главный критерий для нас: вес модели и её скорость работы. В последнее время sota-моделями являются трансформеры, которые я использую в этой работе. Её скорость зависит от размера последовательности в квадрате, но её можно распараллелить, в отличии от LSTM. Также плюс трансформера в интерпретируемости модели, механизм внимания позволяет понять, на какие слова модель обращает внимание.

В качестве нижней оценки качества я использую Tf-Idf с логистической регрессией(ноутбук baseline). Качество такого pipeline:
|||
|-----------------|---------|
|precision micro  | 0.604107|
|precision macro  | 0.296288|
|recall micro     | 0.128799|
|recall macro     | 0.066344|
|f1 micro         | 0.212291|
|f1 macro         | 0.107423|


## Как уменьшить модель
Существуют различные способы уменьшения модели, как и количества параметров, так и общий вес:

1. Обучать с числами двойной точности, после обучения использовать числа с одинарной точностью. Такая техника применяется для загрузки моделей в мобильные телефоны.
2. После обучения определить элементы матрицы, не сильно влияющие на конечный результат. Обнулить их.(Sparse networks)
3. Использовать полноразмерную модель для обучения модели поменьше. Knowledge Distilation.

Исходя из имеющихся ресурсов и времени, а также после просмотра ноутбуков на kaggle(где большинство использовали относительно простые модели), я подумал, что можно реализовать следующее: найти минимальное число параметров, при котором модель уже достаточно хорошо(хотя бы в 2 раза выше метрики baseline) решает задачу. У данного решения свои недостатки: большая вероятность упасть в локальный минимум и недообучение, также задача может плохо решаться маленькими моделями.

## Модель

### Предобработка данных

Вместо обычно используемых word2vec как Glove и FastText, занимающие много памяти, я решил использовать свой, с меньшим словарем. Для этого я применяю BPE кодировщик. Также происходит очистка текста от знаков препинания, оставляю только слова и цифры(просто применяю `re.findall(r'[\w]+')`).

### Трансформер

Для трансформера я использую код из [этого ресурса](https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5). Для классификации потребуется только Encoder для вытаскивания фич.

В [статье](https://arxiv.org/pdf/1508.04025.pdf) показаны различные реализации механизма внимания (3 стр.). Для уменьшения количества параметров я меняю реализацию score функции на dot.

### Обучение

Для ускорения обучения я использую последовательность длинной 100 токенов. Это чуть больше половины датасета, остальную часть я делю по 200 токенов. Это может сказаться на обучении, но так как распределения длин примерно одинаковы, то это не должно сильно испортить модель. 

Для работы с имбалансностью датасета использовалось увеличения точек с ненулевым таргетом, и уменьшение точек с нулевым таргетом.

В осносном обучение происходило при 30-100 эпохах, для каждой модели подбирался learning rate(в пределах $10^{-4}-10^{-6}$), который линейно уменьшался во время обучения.

## Результат

Маленьким моделям не удается быть лучше чем baseline:
|||
|-----------------|---------|
|precision micro  | 0.0914  |
|precision macro  | 0.0379  |
|recall micro     | 0.4392  |
|recall macro     | 0.1771  |

Использовались модели с небольшим размерами матриц(<20). Количество параметром было в пределе 200К. Ноутбук, используемый в эксперименте, прилагается(BaseAttention).

## Вывод
Как показывает эксперимент, для решения этой задачи не достаточно использовать маленькие модели, обученные с нуля. Возможно, методы оптимизации большим моделей лучше покажут себя при решении этой задачи.