{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NOk99_8liroG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from catalyst.dl import SupervisedRunner\n",
    "from catalyst.dl.callbacks import AUCCallback, F1ScoreCallback\n",
    "from catalyst.contrib.schedulers import OneCycleLR\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import youtokentome as yttm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import copy\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c3Vcy7txiroO"
   },
   "outputs": [],
   "source": [
    "r = re.compile(r'[\\w]+')\n",
    "BPE_model = yttm.BPE('BPE_10000.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iNI3r6IFiroV"
   },
   "outputs": [],
   "source": [
    "# В google colab стоит pytorch версии 1.1.0, в котором ещё не были добалены трансформеры. \n",
    "# Поэтому используется чужой код. Ссылка в отчете\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 200):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = np.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
    "                pe[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * np.sqrt(self.d_model)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False).cuda()\n",
    "        return x\n",
    "\n",
    "    \n",
    "def attention(q, k, v, d_model, dropout=None):\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) /  np.sqrt(d_model) \n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        scores = dropout(scores)\n",
    "        \n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n",
    "\n",
    "    \n",
    "# Изменен алгоритм подсчета score на dot\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        bs = q.size(0)\n",
    "\n",
    "        k = k.view(bs, -1, self.d_model).transpose(1,2)\n",
    "        q = q.view(bs, -1, self.d_model).transpose(1,2)\n",
    "        v = v.view(bs, -1, self.d_model).transpose(1,2)\n",
    "        scores = attention(q, k, v, self.d_model, self.dropout)\n",
    "        \n",
    "        output = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
    "    \n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        self.linear_1 = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "    \n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "        self.size = d_model\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6GGrlYlvirob"
   },
   "outputs": [],
   "source": [
    "# Чужой код из той же статьи.\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = Attention(d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, output_shape):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.N = N\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(AttentionLayer(d_model), N)\n",
    "        self.norm = Norm(d_model)\n",
    "        self.output_shape = output_shape\n",
    "        self.output_1 = nn.Linear(d_model, d_model)\n",
    "        self.output_2 = nn.Linear(d_model, output_shape)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, src):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x)\n",
    "        x = self.norm(x)\n",
    "        x = torch.sum(self.output_1(x), dim=1)\n",
    "        x = self.sigmoid(self.output_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mBBSQ99Xiroi"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "X = BPE_model.encode([' '.join(r.findall(i.lower())) for i in train['comment_text'].values])\n",
    "y = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values.tolist()\n",
    "\n",
    "# Для ускорения работы сокращаем длину до 200 токен\n",
    "new_X = []\n",
    "new_y = []\n",
    "for x, y_i in zip(X, y):\n",
    "    if len(x) > 200:\n",
    "        for i in range(0, len(x), 200):\n",
    "            new_X.append(x[i:min(200+i, len(x))])\n",
    "            new_y.append(y_i)\n",
    "    else:\n",
    "        new_X.append(x)\n",
    "        new_y.append(y_i)\n",
    "\n",
    "X = new_X\n",
    "y = [torch.FloatTensor(y_) for y_ in y]\n",
    "\n",
    "Xy = list(zip(X, y))\n",
    "shuffle(Xy)\n",
    "X, y = [i[0] for i in Xy], [i[1] for i in Xy]\n",
    "\n",
    "max_len = max([len(i) for i in X])\n",
    "new_X = np.zeros((len(X), max_len))\n",
    "\n",
    "for i, x in enumerate(X):\n",
    "    new_X[i, :len(x)] += x\n",
    "    \n",
    "X = [torch.LongTensor(x) for x in new_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "colab_type": "code",
    "id": "PHlxMu0tirop",
    "outputId": "40f236b1-1197-4800-9333-22df74d7372b"
   },
   "outputs": [],
   "source": [
    "runner = SupervisedRunner()\n",
    "\n",
    "# Веса полученны из LookAtData\n",
    "weights = 143346 / np.array([15294, 1595, 8449, 478, 7877, 1405])\n",
    "weights = torch.FloatTensor(weights).cuda()\n",
    "\n",
    "X_train, X_test = X[:-len(X)//5], X[-len(X)//5:]\n",
    "y_train, y_test = y[:-len(X)//5], y[-len(X)//5:]\n",
    "\n",
    "loader_train = list(zip(X_train, y_train))\n",
    "loader_test = list(zip(X_test, y_test))\n",
    "\n",
    "loader_train = DataLoader(loader_train, batch_size=2**10, shuffle=True)\n",
    "loader_test = DataLoader(loader_test, batch_size=2**10, shuffle=True)\n",
    "\n",
    "model = AttentionModel(10000, 16, 4, 6) \n",
    "# Размер словаря: 10000 токенов, размерность модели(размер вектора после embedding): 16,\n",
    "# количество attention слоев: 4\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = OneCycleLR(optimizer,\n",
    "    num_steps=30, \n",
    "    lr_range=(0.00001, 0.000001),\n",
    "    warmup_steps=10)\n",
    "\n",
    "runner.train(model=model, criterion=nn.BCELoss(reduction='mean', weight=weights),\n",
    "             scheduler=scheduler, optimizer=optimizer,\n",
    "             loaders={'train': loader_train, 'valid': loader_test}, \n",
    "             num_epochs=30, verbose=False, logdir='logs', \n",
    "             callbacks=[F1ScoreCallback(activation='none'), AUCCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UD0ajOLUirou",
    "outputId": "50b5d146-4dd1-4f7f-e567-2e2084ab2780"
   },
   "outputs": [],
   "source": [
    "# Считаем количество параметром модели\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zU_Z2kB6iroy"
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "model.eval()\n",
    "model.cuda()\n",
    "with torch.no_grad():\n",
    "    for x in X[-len(X)//5:]:\n",
    "        x = x.cuda()\n",
    "        y_pred.append((model(x.view(1, -1))).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "uL3M84hCiro3",
    "outputId": "a4eac86f-abf4-472a-c205-167a8410842d"
   },
   "outputs": [],
   "source": [
    "# precision micro\n",
    "precision_score((np.array([i.numpy() for i in y_test]) > 0) * 1, (np.round(np.array(y_pred).reshape(-1, 6)) > 0) * 1, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Tv9gwoXNiro7",
    "outputId": "2c23908d-05fb-4a70-8e53-5e683dbfcb9c"
   },
   "outputs": [],
   "source": [
    "# precision macro\n",
    "precision_score((np.array([i.numpy() for i in y_test]) > 0) * 1, (np.round(np.array(y_pred).reshape(-1, 6)) > 0) * 1, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4HGu1Al0irpE",
    "outputId": "8c9fecc0-f794-4c42-f522-63c61ed72c19"
   },
   "outputs": [],
   "source": [
    "# recall micro\n",
    "recall_score((np.array([i.numpy() for i in y_test]) > 0) * 1, (np.round(np.array(y_pred).reshape(-1, 6)) > 0) * 1, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "B6WGhu10irpM",
    "outputId": "cff4147c-216c-4cc1-aaf9-f80de3093a6d"
   },
   "outputs": [],
   "source": [
    "# recall macro\n",
    "recall_score((np.array([i.numpy() for i in y_test]) > 0) * 1, (np.round(np.array(y_pred).reshape(-1, 6)) > 0) * 1, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6vsvW5RpydU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "BaseAttention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
